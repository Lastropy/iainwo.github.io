---
layout: single
title: "2019-Week22"
author_profile: true
headline: Week 22 of 2019!
---

This week I did a lot of Kaggle.

I was feeling like I was getting burnt out a bit with the Math;
and I think the reason for that was because of the way I was studying.
My studying didn't have a plan to it. Rather it's direction was dictated by my familiarity of the topics I was researching.

Over the week, I did these three micro-courses:<br>

| No. | Course |
| --- | --- |
| 1 | [Kaggle: Intro to Machine Learning Micro-Course](https://www.kaggle.com/learn/intro-to-machine-learning) |
| 2 | [Kaggle: Intermediate Machine Learning Micro-Course](https://www.kaggle.com/learn/intermediate-machine-learning)|
| 3 | [Kaggle: Deep Learning Micro-Course](https://www.kaggle.com/learn/deep-learning) |

I also took a couple minutes to read a page on,<br>

| No. | Topic |
| --- | --- |
| 1 | Hyperparameter Optimization - Grid Search, Random Search, Bayesian Optimizatian, Gradient-Based Optimization, Evolutionary Optimization, Population-Based |
| 2 | Stochastic Gradient Descent (SGD) & Rectified Linear Unit (ReLu) |
    
I wrote these notes,<br>

| No. | Note |
| --- | --- |
| 1 | [2019-JUN-02 10:44 - DL Stochastic Gradient Descent, Backpropagation & ReLu.pdf](/assets/notes/ML/DL_%20Stochastic%20Gradient%20Descent%2C%20Backpropagation%20%26%20ReLu.pdf) |
| 2 | [2019-JUN-02 08:51 - Transfer Learning.pdf](/assets/notes/ML/Transfer%20Learning.pdf) |
| 3 | [2019-JUN-01 21:16 - Convolutions.pdf](/assets/notes/ML/Convolutions.pdf) |
| 4 | [2019-JUN-01 13:09 - Data Leakage.pdf](/assets/notes/ML/Data%20Leakage.pdf) |
| 5 | [2019-MAY-31 16:53 - Hyperparameter Optimization.pdf](/assets/notes/ML/Hyperparameter%20Optimization.pdf) |

Here is how I broke those tasks down by the day,<br>
![What I did Week#22](/assets/images/goals/2019Week22_calendar.png)

I think what was demoralizing for me was that in learning about these topics there was so much that I wasn't familiar with. Repeatedly, I found myself researching concepts I wasn't familiar with - in a process that was nearly always recursive. That recursive process - where I looked up one text then it's dependendent concepts - inorder to clarify the original concept more often than not was exponential in nature. I think what I am realizing is that without propers bounds and without proper goals which to defines those bounds - it is almost inevitable that my learning will devolve into one hot mess. 

Thus, I think the solution is to define good goals. And by that I mean specific, small-in-effort, achievable tasks, that are short term (within weeks or months) and which have timelines to them.

Here is a sketch of what I plan to do over the next three months,<br>
![Short-Term Goals.pdf](/assets/notes/Goals/Short-Term%20Goals.pdf)

Till next week,<br>
Iain
